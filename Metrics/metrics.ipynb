{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb23b3-6940-41c3-967b-3604805fcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93796889-fa8a-4e0d-9afc-d147b52c86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('./tf-gpu/my_model_finetuned.h5')\n",
    "\n",
    "column_names = ['image_id'] + [\n",
    "    \"5_o_Clock_Shadow\", \"Arched_Eyebrows\", \"Attractive\", \"Bags_Under_Eyes\", \"Bald\", \"Bangs\",\n",
    "    \"Big_Lips\", \"Big_Nose\", \"Black_Hair\", \"Blond_Hair\", \"Blurry\", \"Brown_Hair\", \"Bushy_Eyebrows\",\n",
    "    \"Chubby\", \"Double_Chin\", \"Eyeglasses\", \"Goatee\", \"Gray_Hair\", \"Heavy_Makeup\", \"High_Cheekbones\",\n",
    "    \"Male\", \"Mouth_Slightly_Open\", \"Mustache\", \"Narrow_Eyes\", \"No_Beard\", \"Oval_Face\", \"Pale_Skin\",\n",
    "    \"Pointy_Nose\", \"Receding_Hairline\", \"Rosy_Cheeks\", \"Sideburns\", \"Smiling\", \"Straight_Hair\",\n",
    "    \"Wavy_Hair\", \"Wearing_Earrings\", \"Wearing_Hat\", \"Wearing_Lipstick\", \"Wearing_Necklace\",\n",
    "    \"Wearing_Necktie\", \"Young\"\n",
    "]\n",
    "\n",
    "# Define a function to preprocess the input image\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(128, 128))  # Resize the image\n",
    "    img_array = image.img_to_array(img)  # Convert the image to a numpy array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize to [0, 1]\n",
    "    return img_array\n",
    "\n",
    "# Define categories\n",
    "categories = ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young']\n",
    "\n",
    "#return image_paths\n",
    "def give_paths(image_directory):\n",
    "    results = []\n",
    "    # Iterate over all files in the given directory\n",
    "    for root, _, files in os.walk(image_directory):\n",
    "        for file in files:\n",
    "            # Check if the file is an image (based on common image extensions)\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                results.append(os.path.join(root, file))\n",
    "    return results\n",
    "    \n",
    "#definition to get predictions for images in specified dir\n",
    "def update_predictions(image_directory, verbose=False):\n",
    "    # Create an empty list to store the results for 'Blond hair' and 'Young'\n",
    "    results = []\n",
    "    images_without_blond_hair = []  \n",
    "    images_not_young = []  \n",
    "    \n",
    "    # Loop over each image in the directory and make predictions\n",
    "    for img_filename in os.listdir(image_directory):\n",
    "        if img_filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(image_directory, img_filename)\n",
    "            \n",
    "            # Preprocess the image\n",
    "            img = preprocess_image(img_path)\n",
    "            \n",
    "            # Get predictions from the model\n",
    "            predictions = model.predict(img,verbose=0)  # This will be a 2D array (1, 5), corresponding to the 5 categories\n",
    "            \n",
    "            # Translate predictions to dataframe values\n",
    "            young_pred = predictions[0][4]  # Prediction for 'Young'\n",
    "            \n",
    "            # Determine Male and Young values based on threshold\n",
    "            young_value = 1 if young_pred > 0.5 else -1\n",
    "            \n",
    "            # Find the index of the highest hair color prediction\n",
    "            hair_predictions = predictions[0][:3]  # First three predictions correspond to hair colors (Black, Blond, Brown)\n",
    "            max_hair_index = np.argmax(hair_predictions)  # Get the index of the highest prediction\n",
    "            if max_hair_index == 1:\n",
    "                blond_value = 1  # Blond hair is set to 1\n",
    "            else:\n",
    "                blond_value = -1\n",
    "            \n",
    "            # Append the prediction results to the list\n",
    "            results.append([img_filename, blond_value, young_value])\n",
    "\n",
    "            # store image paths\n",
    "            if blond_value == -1:\n",
    "                images_without_blond_hair.append(img_path)\n",
    "            if young_value == -1:\n",
    "                images_not_young.append(img_path)\n",
    "                \n",
    "            # Optionally print the progress if verbose=True\n",
    "            if verbose:\n",
    "                print(f\"Processed: {img_filename} | Blond hair: {blond_value}, Young: {young_value}\")\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame(results, columns=['Image Filename', 'Blond hair', 'Young'])\n",
    "    \n",
    "    # Return the DataFrame with the predictions\n",
    "    return df, images_without_blond_hair, images_not_young\n",
    "\n",
    "\n",
    "# Function to display images in a grid (5 images per row)\n",
    "def display_images(images_paths):\n",
    "    # Number of images per row\n",
    "    images_per_row = 5\n",
    "    num_images = len(images_paths)\n",
    "    \n",
    "    # Calculate number of rows needed to display all images\n",
    "    num_rows = math.ceil(num_images / images_per_row)\n",
    "    \n",
    "    # Create a figure for the grid\n",
    "    fig, axes = plt.subplots(num_rows, images_per_row, figsize=(images_per_row * 3, num_rows * 3))\n",
    "    \n",
    "    # Flatten the axes array to make indexing easier\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over each image and display it in the grid\n",
    "    for i, img_path in enumerate(images_paths):\n",
    "        img = mpimg.imread(img_path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')  # Hide axes for better display\n",
    "    \n",
    "    # Hide any unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    # Adjust the layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to convert an image array to a byte array\n",
    "def image_to_bytes(img_path):\n",
    "    \"\"\"\n",
    "    Converts an image from a file path to a byte array.\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): The path to the image file.\n",
    "        \n",
    "    Returns:\n",
    "        bytes: The byte data of the image.\n",
    "    \"\"\"\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert(\"RGBA\")\n",
    "    \n",
    "    # Create a BytesIO buffer to store the image\n",
    "    img_buffer = BytesIO()\n",
    "    # Save the image in the buffer as PNG format\n",
    "    img.save(img_buffer, format=\"PNG\")\n",
    "    img_buffer.seek(0)  # Rewind the buffer to the beginning\n",
    "    \n",
    "    return img_buffer.read()  # Return the byte data of the image\n",
    "    \n",
    "def interactive_display_images(original_images_path, transformed_images_path, png_true, sample_size=None):\n",
    "    \"\"\"\n",
    "    Displays images side by side (original and transformed) with sliders above each image pair.\n",
    "    \n",
    "    Args:\n",
    "        original_images_path (str): Path to the directory containing original images.\n",
    "        transformed_images_path (list): List of paths to transformed images.\n",
    "        sample_size (int, optional): Number of images to sample from transformed_images_path. Defaults to None (all images).\n",
    "    \n",
    "    Returns:\n",
    "        list: Collected slider values.\n",
    "    \"\"\"\n",
    "    # Sample the transformed images if sample_size is provided\n",
    "    if sample_size:\n",
    "        transformed_images_path = random.sample(transformed_images_path, min(sample_size, len(transformed_images_path)))\n",
    "\n",
    "    # Slider value storage\n",
    "    slider_values = []\n",
    "\n",
    "    # Create the interactive layout\n",
    "    output = widgets.Output()\n",
    "    sliders = []\n",
    "\n",
    "    for i, transformed_img_path in enumerate(transformed_images_path):\n",
    "        # Extract the base filename (e.g., 086706.jpg)\n",
    "        if png_true == 1:\n",
    "            print(\"1\")\n",
    "            print(os.path.basename(transformed_img_path).split('_')[0])\n",
    "            if '_' in os.path.basename(transformed_img_path):\n",
    "                original_img_path = os.path.join(\n",
    "                original_images_path,\n",
    "                os.path.basename(transformed_img_path).split('_')[0] + \".png\"\n",
    "                )\n",
    "            else:\n",
    "                original_img_path = os.path.join(\n",
    "                    original_images_path,\n",
    "                    os.path.basename(transformed_img_path).split('.')[0] + \".png\"\n",
    "                )\n",
    "        if png_true == 0:\n",
    "            print(\"0\")\n",
    "            print(os.path.basename(transformed_img_path).split('_')[0])\n",
    "            if '_' in os.path.basename(transformed_img_path):\n",
    "                original_img_path = os.path.join(\n",
    "                original_images_path,\n",
    "                os.path.basename(transformed_img_path).split('_')[0] + \".jpg\"\n",
    "                )\n",
    "            else:\n",
    "                original_img_path = os.path.join(\n",
    "                    original_images_path,\n",
    "                    os.path.basename(transformed_img_path).split('.')[0] + \".jpg\"\n",
    "                )        \n",
    "        print(original_img_path)\n",
    "        # Create the Image widgets with defined width and height\n",
    "        image_widget1 = widgets.Image(value=image_to_bytes(original_img_path), format='png', width=150, height=150)\n",
    "        image_widget2 = widgets.Image(value=image_to_bytes(transformed_img_path), format='png', width=150, height=150)\n",
    "\n",
    "        # Create a slider\n",
    "        slider = widgets.IntSlider(value=0, min=0, max=10, step=1, description='Value:')\n",
    "        sliders.append(slider)\n",
    "        \n",
    "        # Create a horizontal box layout to display the image, slider, and image\n",
    "        box = widgets.HBox([image_widget1, slider, image_widget2])\n",
    "\n",
    "        # Display the box\n",
    "        display(box)\n",
    "\n",
    "    # Define a button to submit slider values\n",
    "    def submit_sliders(b):\n",
    "        nonlocal slider_values\n",
    "        slider_values = [slider.value for slider in sliders]\n",
    "        \n",
    "        # Now, display the collected slider values using the Output widget\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Slider values collected:\", slider_values)\n",
    "\n",
    "    submit_button = widgets.Button(description=\"Submit\")\n",
    "    submit_button.on_click(submit_sliders)\n",
    "    \n",
    "    # Display the button below the sliders\n",
    "    display(output)\n",
    "    display(submit_button)\n",
    "\n",
    "    return slider_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844cd90-647d-4d07-bef3-dfc29c785b62",
   "metadata": {},
   "source": [
    "**Quantitative analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbc28d-d4cb-4a44-9f4d-0e44f5209b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy for CelebA single transformation\n",
    "#normally this is the number correct predictions made by the classifier. \n",
    "#Here the expected output for all images is \"blond hair,\" and the classifier's prediction for each image is compared to this expected output.\n",
    "# accuracy = Number of correct predictions/Total number of predictions\n",
    "\n",
    "# Call the update_predictions function to get the DataFrame\n",
    "df_filtered, CelebA_ST_images_without_blond_hair, _  = update_predictions(\"./celeba_Orig/reduced/images/single transformation/images\", verbose=False)\n",
    "\n",
    "# Count the number of items (rows) in the DataFrame\n",
    "item_count = len(df_filtered)\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of items in df_filtered: {item_count}\")\n",
    "\n",
    "# Compare the predicted attribute ('Blond_hair') with the expected attribute\n",
    "correct_predictions = df_filtered['Blond hair'] == 1  \n",
    "correct_count = correct_predictions.sum()  # Sum up all True values (1's)\n",
    "\n",
    "# Print the count of correct predictions\n",
    "print(f\"Number of correct predictions: {correct_count}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions.mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#print images\n",
    "#display_images(CelebA_ST_images_without_blond_hair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36720b-bab1-4a5d-a37a-e72e398346d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy for FFHQ single transformation\n",
    "\n",
    "# Call the update_predictions function to get the DataFrame\n",
    "df_filtered, FFHQ_ST_images_without_blond_hair, _  = update_predictions(\"./ffhq/reduced/images/single transformation/images\",verbose=False)\n",
    "\n",
    "# Count the number of items (rows) in the DataFrame\n",
    "item_count = len(df_filtered)\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of items in df_filtered: {item_count}\")\n",
    "\n",
    "# Compare the predicted attribute ('Blond_hair') with the expected attribute\n",
    "correct_predictions = df_filtered['Blond hair'] == 1  \n",
    "correct_count = correct_predictions.sum()  # Sum up all True values (1's)\n",
    "\n",
    "# Print the count of correct predictions\n",
    "print(f\"Number of correct predictions: {correct_count}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions.mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#print images\n",
    "#display_images(FFHQ_ST_images_without_blond_hair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ebf0e5-bb93-46dc-80a7-240b31499c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy for CelebA multi transformation\n",
    "\n",
    "# Call the update_predictions function to get the DataFrame\n",
    "df_filtered,CelebA_MT_images_without_blond_hair,CelebA_MT_images_not_young = update_predictions(\"./celeba_Orig/reduced/images/multi transformation/images\", verbose=False)\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of items in df_filtered: {item_count}\")\n",
    "\n",
    "# Expected values are 1 for both \"Blond hair\" and \"Young\"\n",
    "# Compare both predicted attributes ('Blond_hair' and 'Young') with the expected value\n",
    "# creation of two boolean Series\n",
    "correct_predictions_blond = df_filtered['Blond hair'] == 1   # Blond hair expected to be 1\n",
    "correct_predictions_young = df_filtered['Young'] == 1        # Young expected to be 1\n",
    "\n",
    "# Calculate accuracy for both Blond hair and Young being correctly predicted\n",
    "correct_predictions = correct_predictions_blond & correct_predictions_young\n",
    "correct_count = correct_predictions.sum()  # Sum up all True values (1's)\n",
    "\n",
    "# Print the count of correct predictions\n",
    "print(f\"Number of correct predictions: {correct_count}\")\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = correct_predictions.mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#print images\n",
    "#print(\"CelebA_MT_images_without_blond_hair\")\n",
    "#display_images(CelebA_MT_images_without_blond_hair)\n",
    "\n",
    "#print(\"CelebA_MT_images_without_blond_hair\")\n",
    "#display_images(CelebA_MT_images_without_blond_hair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756350c-1885-451c-9e43-6802120a8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy for ffhq multi transformation\n",
    "\n",
    "# Call the update_predictions function to get the DataFrame\n",
    "df_filtered,FFHQ_MT_images_without_blond_hair,FFHQ_MT_images_not_young = update_predictions(\"./ffhq/reduced/images/multi transformation/images\", verbose=False)\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of items in df_filtered: {item_count}\")\n",
    "\n",
    "# Expected values are 1 for both \"Blond hair\" and \"Young\"\n",
    "# Compare both predicted attributes ('Blond_hair' and 'Young') with the expected value\n",
    "# creation of two boolean Series\n",
    "correct_predictions_blond = df_filtered['Blond hair'] == 1   # Blond hair expected to be 1\n",
    "correct_predictions_young = df_filtered['Young'] == 1        # Young expected to be 1\n",
    "\n",
    "# Calculate accuracy for both Blond hair and Young being correctly predicted\n",
    "correct_predictions = correct_predictions_blond & correct_predictions_young\n",
    "correct_count = correct_predictions.sum()  # Sum up all True values (1's)\n",
    "\n",
    "# Print the count of correct predictions\n",
    "print(f\"Number of correct predictions: {correct_count}\")\n",
    "\n",
    "# Calculate overall accuracy -  computes the proportion of True values in the correct_predictions series\n",
    "accuracy = correct_predictions.mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#print images\n",
    "#print(\"FFHQ_MT_images_without_blond_hair\")\n",
    "#display_images(FFHQ_MT_images_without_blond_hair)\n",
    "\n",
    "#print(\"FFHQ_MT_images_without_blond_hair\")\n",
    "#display_images(FFHQ_MT_images_without_blond_hair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510065cd-ca94-4aeb-b55a-7ec8ab25b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy side by side\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the accuracy results\n",
    "data = {\n",
    "    'Transformation': ['CelebA Single', 'FFHQ Single', 'CelebA Multi', 'FFHQ Multi'],\n",
    "    'Accuracy (%)': [89.04, 86.44, 75.64, 70.84]  # These should be the accuracy values you calculated\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df_accuracy = pd.DataFrame(data)\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Transformation', y='Accuracy (%)', data=df_accuracy, palette='Blues_d')\n",
    "plt.title('Accuracy Comparison: CelebA vs FFHQ', fontsize=16)\n",
    "plt.xlabel('Transformation Type', fontsize=14)\n",
    "plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "plt.ylim(0, 100)  # Set y-axis limits from 0 to 100\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table (this can be included in the paper if needed)\n",
    "print(\"Accuracy Table:\")\n",
    "print(df_accuracy.to_string(index=False))\n",
    "\n",
    "# If you want to save the table to a file\n",
    "df_accuracy.to_csv('accuracy_comparison.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b949f879-a00c-4378-bc82-397459ec93f5",
   "metadata": {},
   "source": [
    "**Qualitative analysis**\n",
    "\n",
    "through visual inspection of the transformed images, assessing whether the generated attributes are consistent with the intended transformation (blond hair, young).Perform Visual Consistency Score: Give each image a score from 1 to 10 based on how well the transformation matches the target attribute (blond hair, young)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc6e26-00d1-4021-861d-5bca4c18e944",
   "metadata": {},
   "source": [
    "Qualitative analysis - scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7669bcd-fea3-46c5-9241-39b33958dcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "141282\n",
      "./celeba_Orig/images\\141282.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c141757a57c6424eb9917c29a77265a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "078568\n",
      "./celeba_Orig/images\\078568.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35cac67c32e74fa696b0318dfe25eb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "068436\n",
      "./celeba_Orig/images\\068436.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6140802242884b8c92df9d849f946e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "050907\n",
      "./celeba_Orig/images\\050907.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8905f9a449b84f2d9a8d75929d964852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "184598\n",
      "./celeba_Orig/images\\184598.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f52ff5b9ee41efb3e6d3f256b27af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "168183\n",
      "./celeba_Orig/images\\168183.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59e1e259a674528b88d21443feb124c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "102942\n",
      "./celeba_Orig/images\\102942.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b986264003418aaec18a0526631b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "171242\n",
      "./celeba_Orig/images\\171242.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ea41b4c7c74d50a080c7730acd567a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "162600\n",
      "./celeba_Orig/images\\162600.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20708be98a54d46b6ba464c6e61057e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "104761\n",
      "./celeba_Orig/images\\104761.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e194c85547a4c35b8d3158411cec1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277efea509ea4ae39e4c757e63caaae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27cbf53e2954181b5fed37ed5d4cd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CelebA single transformation (original images side by side with transformed images, sample size 10) - evaluate blond hair\n",
    "slider_values_CelebA_ST = interactive_display_images(\"./celeba_Orig/images\",give_paths(\"./celeba_Orig/reduced/images/single transformation/images\"), 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf23e34e-d2e4-40cf-be47-d8da2e15b468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "05015\n",
      "./ffhq/images\\05015.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09fd61e4a90049369eb981a5619723f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "09619\n",
      "./ffhq/images\\09619.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30468c6e70d14526a55eeaf42c29c60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "05250\n",
      "./ffhq/images\\05250.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3938c4d1fe47488fbc1d4a2862c58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "09296\n",
      "./ffhq/images\\09296.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab125bf293b8470489f4446a30715f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "17255\n",
      "./ffhq/images\\17255.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdba32e48aa44518bd2c90b399bd02c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "03100\n",
      "./ffhq/images\\03100.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e73d7a9f26b400eb161c53422803522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "09324\n",
      "./ffhq/images\\09324.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979da2c14fbf45b2bc66d8a7cdf7a4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "08796\n",
      "./ffhq/images\\08796.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f12ef921fa4ebd87b4d3f8a1a8986a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "03510\n",
      "./ffhq/images\\03510.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f55b216987b43a3bf3e7a23c55bf27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "06754\n",
      "./ffhq/images\\06754.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b315a927b8b4d3a924b69833c1c0bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69d833fb2c341cf8422fa6c56198267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2f3c99ea684e4f9381d1dca2125075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FFHQ single transformation (original images side by side with transformed images, sample size 10)  - evaluate blond hair\n",
    "slider_values_ffhq_ST = interactive_display_images(\"./ffhq/images\",give_paths(\"./ffhq/reduced/images/single transformation/images\"), 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "100ec49c-7acc-4f01-82d8-b61ab551490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "087211.jpg\n",
      "./celeba_Orig/images\\087211.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79736c65b282486ba69ae0d0b1e1106a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "126265.jpg\n",
      "./celeba_Orig/images\\126265.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2affa052b64c63b0ec9461b8f575f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "161431.jpg\n",
      "./celeba_Orig/images\\161431.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cb7372eeeb4a3caed170f22ae4e872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "123550.jpg\n",
      "./celeba_Orig/images\\123550.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc40fcf66b3e4be1aa453f8873128f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "155685.jpg\n",
      "./celeba_Orig/images\\155685.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af38fbaa6014590a54fa167a139d60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "083096.jpg\n",
      "./celeba_Orig/images\\083096.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8a4e979f484174b5fbbfff49a22057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "178665.jpg\n",
      "./celeba_Orig/images\\178665.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ea7b73e4814791bef88e6d72045cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "143651.jpg\n",
      "./celeba_Orig/images\\143651.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c585c73897d44eb862c4534780af4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "143423.jpg\n",
      "./celeba_Orig/images\\143423.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a391dc8f10a148dbb24a838ed3f8167c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "051247.jpg\n",
      "./celeba_Orig/images\\051247.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6c891d9802487f960ff19969268d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xb2\\x00\\x00\\x00\\xda\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51058f97b1047b5bd5858c54415ca8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26d903996024f1a9e7551a782cbd499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CelebA multi transformation (original images side by side with transformed images, sample size 10) - evaluate rejuvenation\n",
    "slider_values_CelebA_MT = interactive_display_images(\"./celeba_Orig/images\",give_paths(\"./celeba_Orig/reduced/images/multi transformation/images\"), 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ae7bb8a-6dd1-486f-abf2-f1b0f38ae316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "01566.jpg\n",
      "./ffhq/images\\01566.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c628f66d544d4cf59d88d69652057607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00831.jpg\n",
      "./ffhq/images\\00831.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec961a1572d04646a3affd88e2e42ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "06182.jpg\n",
      "./ffhq/images\\06182.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e22c81f02574a34adc34bb473d8afdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "08704.jpg\n",
      "./ffhq/images\\08704.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d6ac2a9cac4f49967d76727a1ec63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "17195.jpg\n",
      "./ffhq/images\\17195.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad499bdba9d4d17ae611515397ea58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11063.jpg\n",
      "./ffhq/images\\11063.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683f24bc92d64bfaa2242fdc7cd53277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "03994.jpg\n",
      "./ffhq/images\\03994.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71fc8da7a04f67864c1cc54776f3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "15386.jpg\n",
      "./ffhq/images\\15386.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b761c66c5914da5984642a32d6ec842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "01948.jpg\n",
      "./ffhq/images\\01948.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1d8e88fd60400ea80ce690b1930a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10891.jpg\n",
      "./ffhq/images\\10891.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa050ce32b640a9be333a6a2e757ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x06\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141c0419da9a42988bc55b74f187044b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283f5ea01759449e87bc0576eb4c3150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FFHQ multi transformation (original images side by side with transformed images, sample size 10) - evaluate rejuvenation\n",
    "slider_values_ffhq_MT = interactive_display_images(\"./ffhq/images\",give_paths(\"./ffhq/reduced/images/multi transformation/images\"), 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12be2191-b875-4564-b323-07de8c883463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scores to csv\n",
    "df_slider_values_CelebA_ST = pd.DataFrame(slider_values_CelebA_ST, columns=['Score'])\n",
    "df_slider_values_ffhq_ST = pd.DataFrame(slider_values_ffhq_ST, columns=['Score'])\n",
    "df_slider_values_CelebA_MT = pd.DataFrame(slider_values_CelebA_MT, columns=['Score'])\n",
    "df_slider_values_ffhq_MT = pd.DataFrame(slider_values_ffhq_MT, columns=['Score'])\n",
    "\n",
    "# Save to a CSV file\n",
    "csv_filename = \"slider_values_CelebA_ST1.csv\"\n",
    "df_slider_values_CelebA_ST.to_csv(csv_filename, index=False)\n",
    "\n",
    "csv_filename = \"slider_values_ffhq_ST1.csv\"\n",
    "df_slider_values_ffhq_ST.to_csv(csv_filename, index=False)\n",
    "\n",
    "csv_filename = \"slider_values_CelebA_MT1.csv\"\n",
    "df_slider_values_CelebA_MT.to_csv(csv_filename, index=False)\n",
    "\n",
    "csv_filename = \"slider_values_ffhq_MT1.csv\"\n",
    "df_slider_values_ffhq_MT.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb0953-4820-444d-9b03-75c6674d290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 2 CSV files (scores by each collaborator) into DataFrames\n",
    "df1 = pd.read_csv(\"slider_values_CelebA_ST1.csv\")\n",
    "df2 = pd.read_csv(\"slider_values_CelebA_ST2.csv\")\n",
    "\n",
    "# Assuming both DataFrames have a common column named 'id'\n",
    "slider_values_CelebA_ST = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Load the 2 CSV files (scores by each collaborator) into DataFrames\n",
    "df1 = pd.read_csv(\"slider_values_ffhq_ST1.csv\")\n",
    "df2 = pd.read_csv(\"slider_values_ffhq_ST2.csv\")\n",
    "\n",
    "# Assuming both DataFrames have a common column named 'id'\n",
    "slider_values_ffhq_ST = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Load the 2 CSV files (scores by each collaborator) into DataFrames\n",
    "df1 = pd.read_csv(\"slider_values_CelebA_MT1.csv\")\n",
    "df2 = pd.read_csv(\"slider_values_CelebA_MT2.csv\")\n",
    "\n",
    "# Assuming both DataFrames have a common column named 'id'\n",
    "slider_values_CelebA_MT = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Load the 2 CSV files (scores by each collaborator) into DataFrames\n",
    "df1 = pd.read_csv(\"slider_values_ffhq_MT1.csv\")\n",
    "df2 = pd.read_csv(\"slider_values_ffhq_MT2.csv\")\n",
    "\n",
    "# Assuming both DataFrames have a common column named 'id'\n",
    "slider_values_ffhq_MT = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff351f7-445c-456d-8e4d-cd2c535f0c57",
   "metadata": {},
   "source": [
    "Qualitative analysis - calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12fb3b5-cef5-45d2-a3be-88966ffa6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CelebA single transformation to blond hair\")\n",
    "# Calculate the mean\n",
    "mean_score = np.mean(slider_values_CelebA_ST)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_deviation = np.std(slider_values_CelebA_ST)\n",
    "\n",
    "print(f\"Mean: {mean_score}\")\n",
    "print(f\"Standard Deviation: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d818c12-ea11-4156-bb2e-a606f8af5f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FFHQ single transformation to blond hair\")\n",
    "# Calculate the mean\n",
    "mean_score = np.mean(slider_values_ffhq_ST)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_deviation = np.std(slider_values_ffhq_ST)\n",
    "\n",
    "print(f\"Mean: {mean_score}\")\n",
    "print(f\"Standard Deviation: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a0d6ee-ae03-42c1-9048-66dba870801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CelebA multi transformation to young\")\n",
    "# Calculate the mean\n",
    "mean_score = np.mean(slider_values_CelebA_MT)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_deviation = np.std(slider_values_CelebA_MT)\n",
    "\n",
    "print(f\"Mean: {mean_score}\")\n",
    "print(f\"Standard Deviation: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a8b57-a2ee-4eb7-829e-2b3c99926f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FFHQ multi transformation to blond hair\")\n",
    "# Calculate the mean\n",
    "mean_score = np.mean(slider_values_ffhq_MT)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_deviation = np.std(slider_values_ffhq_MT)\n",
    "\n",
    "print(f\"Mean: {mean_score}\")\n",
    "print(f\"Standard Deviation: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492dd7d5-dff3-4f8e-bb5e-88a630475a4e",
   "metadata": {},
   "source": [
    "Qualitative analysis - histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267c999-7f92-47f3-a9cf-86eca1388dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histograms\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10), sharex=True, sharey=True)\n",
    "\n",
    "# CelebA Single Transformation (ST)\n",
    "axes[0, 0].hist(slider_values_CelebA_ST, bins=10, range=(0, 10), color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('CelebA - Blond Hair Transformation (ST)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# FFHQ Single Transformation (ST)\n",
    "axes[0, 1].hist(slider_values_ffhq_ST, bins=10, range=(0, 10), color='green', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('FFHQ - Blond Hair Transformation (ST)')\n",
    "axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# CelebA Multiple Transformation (MT)\n",
    "axes[1, 0].hist(slider_values_CelebA_MT, bins=10, range=(0, 10), color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('CelebA - Blond Hair + Young Transformation (MT)')\n",
    "axes[1, 0].set_xlabel('Scores')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# FFHQ Multiple Transformation (MT)\n",
    "axes[1, 1].hist(slider_values_ffhq_MT, bins=10, range=(0, 10), color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('FFHQ - Blond Hair + Young Transformation (MT)')\n",
    "axes[1, 1].set_xlabel('Scores')\n",
    "axes[1, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9af170-6c74-4fc6-8b04-5be70818de40",
   "metadata": {},
   "source": [
    "The histograms will reveal:\n",
    "Distribution shape (e.g., normal, skewed).\n",
    "Variability in ratings (spread of scores).\n",
    "Differences in scoring patterns between single and multiple transformations or between datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
