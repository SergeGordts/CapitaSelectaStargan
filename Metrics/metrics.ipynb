{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb23b3-6940-41c3-967b-3604805fcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93796889-fa8a-4e0d-9afc-d147b52c86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('./tf-gpu/my_model_finetuned.h5')\n",
    "\n",
    "column_names = ['image_id'] + [\n",
    "    \"5_o_Clock_Shadow\", \"Arched_Eyebrows\", \"Attractive\", \"Bags_Under_Eyes\", \"Bald\", \"Bangs\",\n",
    "    \"Big_Lips\", \"Big_Nose\", \"Black_Hair\", \"Blond_Hair\", \"Blurry\", \"Brown_Hair\", \"Bushy_Eyebrows\",\n",
    "    \"Chubby\", \"Double_Chin\", \"Eyeglasses\", \"Goatee\", \"Gray_Hair\", \"Heavy_Makeup\", \"High_Cheekbones\",\n",
    "    \"Male\", \"Mouth_Slightly_Open\", \"Mustache\", \"Narrow_Eyes\", \"No_Beard\", \"Oval_Face\", \"Pale_Skin\",\n",
    "    \"Pointy_Nose\", \"Receding_Hairline\", \"Rosy_Cheeks\", \"Sideburns\", \"Smiling\", \"Straight_Hair\",\n",
    "    \"Wavy_Hair\", \"Wearing_Earrings\", \"Wearing_Hat\", \"Wearing_Lipstick\", \"Wearing_Necklace\",\n",
    "    \"Wearing_Necktie\", \"Young\"\n",
    "]\n",
    "\n",
    "# Define a function to preprocess the input image\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(128, 128))  # Resize the image\n",
    "    img_array = image.img_to_array(img)  # Convert the image to a numpy array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize to [0, 1]\n",
    "    return img_array\n",
    "\n",
    "# Define categories\n",
    "categories = ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young']\n",
    "\n",
    "#return image_paths\n",
    "def give_paths(image_directory):\n",
    "    results = []\n",
    "    # Iterate over all files in the given directory\n",
    "    for root, _, files in os.walk(image_directory):\n",
    "        for file in files:\n",
    "            # Check if the file is an image (based on common image extensions)\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                results.append(os.path.join(root, file))\n",
    "    return results\n",
    "    \n",
    "#definition to get predictions for images in specified dir\n",
    "def update_predictions(image_directory, verbose=False):\n",
    "    # Create an empty list to store the results for 'Blond hair' and 'Young'\n",
    "    results = []\n",
    "    images_without_blond_hair = []  \n",
    "    images_not_young = []  \n",
    "    \n",
    "    # Loop over each image in the directory and make predictions\n",
    "    for img_filename in os.listdir(image_directory):\n",
    "        if img_filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(image_directory, img_filename)\n",
    "            \n",
    "            # Preprocess the image\n",
    "            img = preprocess_image(img_path)\n",
    "            \n",
    "            # Get predictions from the model\n",
    "            predictions = model.predict(img,verbose=0)  # This will be a 2D array (1, 5), corresponding to the 5 categories\n",
    "            \n",
    "            # Translate predictions to dataframe values\n",
    "            young_pred = predictions[0][4]  # Prediction for 'Young'\n",
    "            \n",
    "            # Determine Male and Young values based on threshold\n",
    "            young_value = 1 if young_pred > 0.5 else -1\n",
    "            \n",
    "            # Find the index of the highest hair color prediction\n",
    "            hair_predictions = predictions[0][:3]  # First three predictions correspond to hair colors (Black, Blond, Brown)\n",
    "            max_hair_index = np.argmax(hair_predictions)  # Get the index of the highest prediction\n",
    "            if max_hair_index == 1:\n",
    "                blond_value = 1  # Blond hair is set to 1\n",
    "            else:\n",
    "                blond_value = -1\n",
    "            \n",
    "            # Append the prediction results to the list\n",
    "            results.append([img_filename, blond_value, young_value])\n",
    "\n",
    "            # store image paths\n",
    "            if blond_value == -1:\n",
    "                images_without_blond_hair.append(img_path)\n",
    "            if young_value == -1:\n",
    "                images_not_young.append(img_path)\n",
    "                \n",
    "            # Optionally print the progress if verbose=True\n",
    "            if verbose:\n",
    "                print(f\"Processed: {img_filename} | Blond hair: {blond_value}, Young: {young_value}\")\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame(results, columns=['Image Filename', 'Blond hair', 'Young'])\n",
    "    \n",
    "    # Return the DataFrame with the predictions\n",
    "    return df, images_without_blond_hair, images_not_young\n",
    "\n",
    "\n",
    "# Function to display images in a grid (5 images per row)\n",
    "def display_images(images_paths):\n",
    "    # Number of images per row\n",
    "    images_per_row = 5\n",
    "    num_images = len(images_paths)\n",
    "    \n",
    "    # Calculate number of rows needed to display all images\n",
    "    num_rows = math.ceil(num_images / images_per_row)\n",
    "    \n",
    "    # Create a figure for the grid\n",
    "    fig, axes = plt.subplots(num_rows, images_per_row, figsize=(images_per_row * 3, num_rows * 3))\n",
    "    \n",
    "    # Flatten the axes array to make indexing easier\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over each image and display it in the grid\n",
    "    for i, img_path in enumerate(images_paths):\n",
    "        img = mpimg.imread(img_path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')  # Hide axes for better display\n",
    "    \n",
    "    # Hide any unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    # Adjust the layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to convert an image array to a byte array\n",
    "def image_to_bytes(img_path):\n",
    "    \"\"\"\n",
    "    Converts an image from a file path to a byte array.\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): The path to the image file.\n",
    "        \n",
    "    Returns:\n",
    "        bytes: The byte data of the image.\n",
    "    \"\"\"\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert(\"RGBA\")\n",
    "    \n",
    "    # Create a BytesIO buffer to store the image\n",
    "    img_buffer = BytesIO()\n",
    "    # Save the image in the buffer as PNG format\n",
    "    img.save(img_buffer, format=\"PNG\")\n",
    "    img_buffer.seek(0)  # Rewind the buffer to the beginning\n",
    "    \n",
    "    return img_buffer.read()  # Return the byte data of the image\n",
    "\n",
    "#giving scores to side-by-side images, save as csv-file\n",
    "def interactive_display_images(original_images_path, transformed_images_path, png_true, sample_size, save_filename):\n",
    "    \"\"\"\n",
    "    Displays images side by side (original and transformed) with sliders above each image pair.\n",
    "    \n",
    "    Args:\n",
    "        original_images_path (str): Path to the directory containing original images.\n",
    "        transformed_images_path (list): List of paths to transformed images.\n",
    "        sample_size (int, optional): Number of images to sample from transformed_images_path. Defaults to None (all images).\n",
    "    \n",
    "    Returns:\n",
    "        list: Collected slider values.\n",
    "    \"\"\"\n",
    "    # Sample the transformed images if sample_size is provided\n",
    "    if sample_size:\n",
    "        transformed_images_path = random.sample(transformed_images_path, min(sample_size, len(transformed_images_path)))\n",
    "\n",
    "    # Input value storage\n",
    "    input_box_values = []\n",
    "    \n",
    "    # Create the interactive layout\n",
    "    output = widgets.Output()\n",
    "    input_boxes = []\n",
    "    \n",
    "    for i, transformed_img_path in enumerate(transformed_images_path):\n",
    "        # Extract the base filename (e.g., 086706.jpg)\n",
    "        if png_true == 1:\n",
    "            if '_' in os.path.basename(transformed_img_path):\n",
    "                original_img_path = os.path.join(\n",
    "                original_images_path,\n",
    "                os.path.basename(transformed_img_path).split('_')[0] + \".png\"\n",
    "                )\n",
    "            else:\n",
    "                original_img_path = os.path.join(\n",
    "                    original_images_path,\n",
    "                    os.path.basename(transformed_img_path).split('.')[0] + \".png\"\n",
    "                )\n",
    "        if png_true == 0:\n",
    "            if '_' in os.path.basename(transformed_img_path):\n",
    "                original_img_path = os.path.join(\n",
    "                original_images_path,\n",
    "                os.path.basename(transformed_img_path).split('_')[0] + \".jpg\"\n",
    "                )\n",
    "            else:\n",
    "                original_img_path = os.path.join(\n",
    "                    original_images_path,\n",
    "                    os.path.basename(transformed_img_path).split('.')[0] + \".jpg\"\n",
    "                )        \n",
    "        # Create the Image widgets with defined width and height\n",
    "        image_widget1 = widgets.Image(value=image_to_bytes(original_img_path), format='png', width=150, height=150)\n",
    "        image_widget2 = widgets.Image(value=image_to_bytes(transformed_img_path), format='png', width=150, height=150)\n",
    "\n",
    "        # Create an input_box\n",
    "        input_box = widgets.IntText(value=0, min=0, max=10, step=1, description='Value:')\n",
    "        input_boxes.append(input_box)\n",
    "        \n",
    "        # Create a horizontal box layout to display the image, slider, and image\n",
    "        box = widgets.HBox([image_widget1, input_box, image_widget2])\n",
    "\n",
    "        # Display the box\n",
    "        display(box)\n",
    "\n",
    "    # Define a button to submit input_box\n",
    "    def submit_input_box(b):\n",
    "        nonlocal input_box_values\n",
    "        input_box_values = [input_box.value for input_box in input_boxes]\n",
    "        df_values = pd.DataFrame(input_box_values, columns=['Score'])\n",
    "\n",
    "        # Save to a CSV file\n",
    "        csv_filename = save_filename\n",
    "        df_values.to_csv(csv_filename, index=False)\n",
    "        \n",
    "        # Now, display the collected values using the Output widget\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"values collected:\", input_box_values)\n",
    "\n",
    "    submit_button = widgets.Button(description=\"Submit\")\n",
    "    submit_button.on_click(submit_input_box)\n",
    "       \n",
    "    # Display the button below the sliders\n",
    "    display(output)\n",
    "    display(submit_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844cd90-647d-4d07-bef3-dfc29c785b62",
   "metadata": {},
   "source": [
    "**Quantitative analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbc28d-d4cb-4a44-9f4d-0e44f5209b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy for CelebA single transformation\n",
    "#normally this is the number correct predictions made by the classifier. \n",
    "#Here the expected output for all images is \"blond hair,\" and the classifier's prediction for each image is compared to this expected output.\n",
    "# accuracy = Number of correct predictions/Total number of predictions\n",
    "\n",
    "# Call the update_predictions function to get the DataFrame\n",
    "df_filtered, CelebA_ST_images_without_blond_hair, _  = update_predictions(\"./celeba_Orig/reduced/images/single transformation/images\", verbose=False)\n",
    "\n",
    "# Count the number of items (rows) in the DataFrame\n",
    "item_count = len(df_filtered)\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of items in df_filtered: {item_count}\")\n",
    "\n",
    "# Compare the predicted attribute ('Blond_hair') with the expected attribute\n",
    "correct_predictions = df_filtered['Blond hair'] == 1  \n",
    "correct_count = correct_predictions.sum()  # Sum up all True values (1's)\n",
    "\n",
    "# Print the count of correct predictions\n",
    "print(f\"Number of correct predictions: {correct_count}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions.mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#print images\n",
    "#display_images(CelebA_ST_images_without_blond_hair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36720b-bab1-4a5d-a37a-e72e398346d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy for FFHQ single transformation\n",
    "\n",
    "# Call the update_predictions function to get the DataFrame\n",
    "df_filtered, FFHQ_ST_images_without_blond_hair, _  = update_predictions(\"./ffhq/reduced/images/single transformation/images\",verbose=False)\n",
    "\n",
    "# Count the number of items (rows) in the DataFrame\n",
    "item_count = len(df_filtered)\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of items in df_filtered: {item_count}\")\n",
    "\n",
    "# Compare the predicted attribute ('Blond_hair') with the expected attribute\n",
    "correct_predictions = df_filtered['Blond hair'] == 1  \n",
    "correct_count = correct_predictions.sum()  # Sum up all True values (1's)\n",
    "\n",
    "# Print the count of correct predictions\n",
    "print(f\"Number of correct predictions: {correct_count}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions.mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#print images\n",
    "#display_images(FFHQ_ST_images_without_blond_hair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ebf0e5-bb93-46dc-80a7-240b31499c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy for CelebA multi transformation\n",
    "\n",
    "# Call the update_predictions function to get the DataFrame\n",
    "df_filtered,CelebA_MT_images_without_blond_hair,CelebA_MT_images_not_young = update_predictions(\"./celeba_Orig/reduced/images/multi transformation/images\", verbose=False)\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of items in df_filtered: {item_count}\")\n",
    "\n",
    "# Expected values are 1 for both \"Blond hair\" and \"Young\"\n",
    "# Compare both predicted attributes ('Blond_hair' and 'Young') with the expected value\n",
    "# creation of two boolean Series\n",
    "correct_predictions_blond = df_filtered['Blond hair'] == 1   # Blond hair expected to be 1\n",
    "correct_predictions_young = df_filtered['Young'] == 1        # Young expected to be 1\n",
    "\n",
    "# Calculate accuracy for both Blond hair and Young being correctly predicted\n",
    "correct_predictions = correct_predictions_blond & correct_predictions_young\n",
    "correct_count = correct_predictions.sum()  # Sum up all True values (1's)\n",
    "\n",
    "# Print the count of correct predictions\n",
    "print(f\"Number of correct predictions: {correct_count}\")\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = correct_predictions.mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#print images\n",
    "#print(\"CelebA_MT_images_without_blond_hair\")\n",
    "#display_images(CelebA_MT_images_without_blond_hair)\n",
    "\n",
    "#print(\"CelebA_MT_images_without_blond_hair\")\n",
    "#display_images(CelebA_MT_images_without_blond_hair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756350c-1885-451c-9e43-6802120a8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy for ffhq multi transformation\n",
    "\n",
    "# Call the update_predictions function to get the DataFrame\n",
    "df_filtered,FFHQ_MT_images_without_blond_hair,FFHQ_MT_images_not_young = update_predictions(\"./ffhq/reduced/images/multi transformation/images\", verbose=False)\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of items in df_filtered: {item_count}\")\n",
    "\n",
    "# Expected values are 1 for both \"Blond hair\" and \"Young\"\n",
    "# Compare both predicted attributes ('Blond_hair' and 'Young') with the expected value\n",
    "# creation of two boolean Series\n",
    "correct_predictions_blond = df_filtered['Blond hair'] == 1   # Blond hair expected to be 1\n",
    "correct_predictions_young = df_filtered['Young'] == 1        # Young expected to be 1\n",
    "\n",
    "# Calculate accuracy for both Blond hair and Young being correctly predicted\n",
    "correct_predictions = correct_predictions_blond & correct_predictions_young\n",
    "correct_count = correct_predictions.sum()  # Sum up all True values (1's)\n",
    "\n",
    "# Print the count of correct predictions\n",
    "print(f\"Number of correct predictions: {correct_count}\")\n",
    "\n",
    "# Calculate overall accuracy -  computes the proportion of True values in the correct_predictions series\n",
    "accuracy = correct_predictions.mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#print images\n",
    "#print(\"FFHQ_MT_images_without_blond_hair\")\n",
    "#display_images(FFHQ_MT_images_without_blond_hair)\n",
    "\n",
    "#print(\"FFHQ_MT_images_without_blond_hair\")\n",
    "#display_images(FFHQ_MT_images_without_blond_hair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510065cd-ca94-4aeb-b55a-7ec8ab25b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy side by side\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the accuracy results\n",
    "data = {\n",
    "    'Transformation': ['CelebA Single', 'FFHQ Single', 'CelebA Multi', 'FFHQ Multi'],\n",
    "    'Accuracy (%)': [89.04, 86.44, 75.64, 70.84]  # These should be the accuracy values you calculated\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df_accuracy = pd.DataFrame(data)\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Transformation', y='Accuracy (%)', data=df_accuracy, palette='Blues_d')\n",
    "plt.title('Accuracy Comparison: CelebA vs FFHQ', fontsize=16)\n",
    "plt.xlabel('Transformation Type', fontsize=14)\n",
    "plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "plt.ylim(0, 100)  # Set y-axis limits from 0 to 100\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table (this can be included in the paper if needed)\n",
    "print(\"Accuracy Table:\")\n",
    "print(df_accuracy.to_string(index=False))\n",
    "\n",
    "# If you want to save the table to a file\n",
    "df_accuracy.to_csv('accuracy_comparison.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b949f879-a00c-4378-bc82-397459ec93f5",
   "metadata": {},
   "source": [
    "**Qualitative analysis**\n",
    "\n",
    "through visual inspection of the transformed images, assessing whether the generated attributes are consistent with the intended transformation (blond hair, young) while maintaining realism and preserving the other attributes. Perform Visual Consistency Score: Give each image a score from 1 to 10 based on how well the transformation matches the target attribute (blond hair, young)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc6e26-00d1-4021-861d-5bca4c18e944",
   "metadata": {},
   "source": [
    "Qualitative analysis - scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7669bcd-fea3-46c5-9241-39b33958dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelebA single transformation (original images side by side with transformed images, sample size 10) - evaluate blond hair\n",
    "interactive_display_images(\"./celeba_Orig/images\",give_paths(\"./celeba_Orig/reduced/images/single transformation/images\"), 0, 2,\"values_CelebA_ST1.csv\")\n",
    "#rightclick on the output cell and choose \"Disable Scrolling for outputs\" to disengage the right hand slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23e34e-d2e4-40cf-be47-d8da2e15b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FFHQ single transformation (original images side by side with transformed images, sample size 10)  - evaluate blond hair\n",
    "interactive_display_images(\"./ffhq/images\",give_paths(\"./ffhq/reduced/images/single transformation/images\"), 1, 200, \"values_ffhq_ST1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ec49c-7acc-4f01-82d8-b61ab551490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelebA multi transformation (original images side by side with transformed images, sample size 10) - evaluate rejuvenation\n",
    "interactive_display_images(\"./celeba_Orig/images\",give_paths(\"./celeba_Orig/reduced/images/multi transformation/images\"), 0, 200,\"values_CelebA_MT1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7bb8a-6dd1-486f-abf2-f1b0f38ae316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FFHQ multi transformation (original images side by side with transformed images, sample size 10) - evaluate rejuvenation\n",
    "interactive_display_images(\"./ffhq/images\",give_paths(\"./ffhq/reduced/images/multi transformation/images\"), 1, 200, \"values_ffhq_ST1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb0953-4820-444d-9b03-75c6674d290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 2 CSV files (scores by each collaborator) into DataFrames\n",
    "df1 = pd.read_csv(\"values_CelebA_ST1.csv\")\n",
    "df2 = pd.read_csv(\"values_CelebA_ST2.csv\")\n",
    "\n",
    "# Assuming both DataFrames have a common column named 'id'\n",
    "values _CelebA_ST = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Load the 2 CSV files (scores by each collaborator) into DataFrames\n",
    "df1 = pd.read_csv(\"values_ffhq_ST1.csv\")\n",
    "df2 = pd.read_csv(\"values_ffhq_ST2.csv\")\n",
    "\n",
    "# Assuming both DataFrames have a common column named 'id'\n",
    "values _ffhq_ST = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Load the 2 CSV files (scores by each collaborator) into DataFrames\n",
    "df1 = pd.read_csv(\"values_CelebA_MT1.csv\")\n",
    "df2 = pd.read_csv(\"values_CelebA_MT2.csv\")\n",
    "\n",
    "# Assuming both DataFrames have a common column named 'id'\n",
    "values _CelebA_MT = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Load the 2 CSV files (scores by each collaborator) into DataFrames\n",
    "df1 = pd.read_csv(\"values_ffhq_MT1.csv\")\n",
    "df2 = pd.read_csv(\"values_ffhq_MT2.csv\")\n",
    "\n",
    "# Assuming both DataFrames have a common column named 'id'\n",
    "values _ffhq_MT = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff351f7-445c-456d-8e4d-cd2c535f0c57",
   "metadata": {},
   "source": [
    "Qualitative analysis - calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12fb3b5-cef5-45d2-a3be-88966ffa6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CelebA single transformation to blond hair\")\n",
    "# Calculate the mean\n",
    "mean_score = np.mean(values_CelebA_ST)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_deviation = np.std(values_CelebA_ST)\n",
    "\n",
    "print(f\"Mean: {mean_score}\")\n",
    "print(f\"Standard Deviation: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d818c12-ea11-4156-bb2e-a606f8af5f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FFHQ single transformation to blond hair\")\n",
    "# Calculate the mean\n",
    "mean_score = np.mean(values_ffhq_ST)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_deviation = np.std(values_ffhq_ST)\n",
    "\n",
    "print(f\"Mean: {mean_score}\")\n",
    "print(f\"Standard Deviation: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a0d6ee-ae03-42c1-9048-66dba870801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CelebA multi transformation to young\")\n",
    "# Calculate the mean\n",
    "mean_score = np.mean(values_CelebA_MT)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_deviation = np.std(values_CelebA_MT)\n",
    "\n",
    "print(f\"Mean: {mean_score}\")\n",
    "print(f\"Standard Deviation: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a8b57-a2ee-4eb7-829e-2b3c99926f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FFHQ multi transformation to blond hair\")\n",
    "# Calculate the mean\n",
    "mean_score = np.mean(values_ffhq_MT)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_deviation = np.std(values_ffhq_MT)\n",
    "\n",
    "print(f\"Mean: {mean_score}\")\n",
    "print(f\"Standard Deviation: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492dd7d5-dff3-4f8e-bb5e-88a630475a4e",
   "metadata": {},
   "source": [
    "Qualitative analysis - histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267c999-7f92-47f3-a9cf-86eca1388dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histograms\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10), sharex=True, sharey=True)\n",
    "\n",
    "# CelebA Single Transformation (ST)\n",
    "axes[0, 0].hist(values_CelebA_ST, bins=10, range=(0, 10), color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('CelebA - Blond Hair Transformation (ST)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# FFHQ Single Transformation (ST)\n",
    "axes[0, 1].hist(values_ffhq_ST, bins=10, range=(0, 10), color='green', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('FFHQ - Blond Hair Transformation (ST)')\n",
    "axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# CelebA Multiple Transformation (MT)\n",
    "axes[1, 0].hist(values_CelebA_MT, bins=10, range=(0, 10), color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('CelebA - Blond Hair + Young Transformation (MT)')\n",
    "axes[1, 0].set_xlabel('Scores')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# FFHQ Multiple Transformation (MT)\n",
    "axes[1, 1].hist(values_ffhq_MT, bins=10, range=(0, 10), color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('FFHQ - Blond Hair + Young Transformation (MT)')\n",
    "axes[1, 1].set_xlabel('Scores')\n",
    "axes[1, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9af170-6c74-4fc6-8b04-5be70818de40",
   "metadata": {},
   "source": [
    "The histograms will reveal:\n",
    "Distribution shape (e.g., normal, skewed).\n",
    "Variability in ratings (spread of scores).\n",
    "Differences in scoring patterns between single and multiple transformations or between datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
